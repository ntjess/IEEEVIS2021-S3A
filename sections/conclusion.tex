\section{Conclusion and Future Work}
The Semi-Supervised Semantic Annotator (S3A) is proposed to address the difficult task of pixel-level annotations of image data. For high-resolution images with numerous complex regions of interest, existing labeling software faces performance bottlenecks attempting to extract ground-truth information. Moreover, there is a lack of capabilities to convert such a labeling workflow into an automated procedure with feedback at every step. Each of these challenges is overcome by various features within S3A specifically designed for such tasks. As a result, S3A provides not only tremendous time savings during ground truth annotation, but also allows an annotation pipeline to be directly converted into a prediction scheme. Furthermore, the rapid feedback accessible at every stage of annotation expedites prototyping of novel solutions to imaging domains in which few examples of prior work exist.

\subsection{Dynamic Algorithm Builder}
Improvements are ongoing and will include a variety of powerful features. Chief among these will be an algorithm builder to allow greater flexibility with mixing and matching useful region modification stages. Consequently, the available selection of algorithms and tweaking capabilities will be drastically increased. Rather than only modifying \emph{parameter} values, users will be able to adaptively add \emph{processing stages} as well.

\subsection{Image Navigation Assistance}
Several aspects of image navigation can be incorporated to simplify  the handling of large images. For instance, a `minimap' tool would allow users to maintain a global image perspective while making local edits. Furthermore, this sense of scale aids intuition of how many regions of similar component density, color, etc. exist within the entire image.

Second, multiple strategies for annotating large images leverage a windowing approach, where they will divide the total image into several smaller pieces in a gridlike fashion. While this has its disadvantages, it is fast, easy to automate, and produces reasonable results depending on the initial image complexity \cite{Vigueras_fullCnnCornealSegmentation}. Hence, these methods would be significantly easier to incorporate into S3A if a generalized windowing framework was incorporated which allows users to specify all necessary parameters such as window overlap, size, sampling frequency, etc.


\subsection{Aggregation of Human Annotation Habits}
Several times, it has been noted that manual segmentation of image data is not a feasible or scalable approach for remotely large datasets. However, there are multiple cases in which human intuition can greatly outperform even complex neural networks, depending on the specific segmentation challenge \cite{Russakovsky_humanCollabAnnotation2015}. For this reason, it would be ideal to capture data points hinting toward this decision-making process and use this information to represent human evaluation of images at scale. This may include taking into account human labeling time per class, hesitation between clicks, relationship between shape boundary complexity and instance quantity, and more. By aggregating such statistics, a pattern may arise which can be leveraged as an additional automated annotation technique.