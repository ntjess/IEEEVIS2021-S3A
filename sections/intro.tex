\section{Introduction}
Labeled image data is essential for tuning and evaluating the performance of machine learning applications.
Such labels are typically defined with approximate enclosing shapes (i.e. simple polygons or parametric shapes), which tend to misrepresent more complex components.
% While this streamlines the labeling process, it misrepresents more complex components.
% When high accuracy is required, labels must be specified at the pixel-level â€“ a process known as segmentation labeling or semantic segmentation.
% A detailed description of this process is explained in \cite{chengSurveyAnalysisAutomatic2018}.

% However, a wide variety of applications require pixel-level accuracy, ranging from hardware assurance to medical imaging.
Alternatively, semantic segmentation is a technique providing pixel-level accuracy which avoids poor foreground representation~\cite{chengSurveyAnalysisAutomatic2018}.
Example fields applying this method include Bill-of-Material (BoM) extraction \cite{azhaganReviewAutomaticBill2019}, quality control during manufacturing \cite{fergusonDetectionSegmentationManufacturing2018,anagnostopoulosComputerVisionApproach2001,anagnostopoulosHighPerformanceComputing2002}, manuscript restoration / digitization \cite{gatosSegmentationfreeRecognitionTechnique2004,kesimanNewSchemeText2016,jainTextSegmentationUsing1992,taxtSegmentationDocumentImages1989,fujisawaSegmentationMethodsCharacter1992}, and effective patient diagnosis \cite{seifertSemanticAnnotationMedical2010,rajchlDeepCutObjectSegmentation2017,yushkevichUserguided3DActive2006,iakovidisRatsnakeVersatileImage2014}.
In all these cases, imprecise annotations severely limit the development of automated solutions and can decrease the accuracy of standard trained segmentation models.

Quality semantic segmentation is difficult due to a reliance on large high-quality datasets, which are often created by manually labeling each image.
Manual annotation is error-prone, costly and greatly hinders scalability.
Several tools have been proposed to alleviate the burden of collecting ground-truth labels for semantic segmentation~\cite{BestImageAnnotation}.
% The more commonly used applications are listed in \cite{BestImageAnnotation}.
Unfortunately, existing tools are heavily biased toward lower-resolution images with few regions of interest (ROI) (Figure~\ref{fig:sampleSegData}).
While this may not be an issue for some datasets, such assumptions are \textit{crippling} for high-fidelity images with hundreds of annotated ROIs (Figure~\ref{fig:bees}).
% This scenario is represented in Figure~\ref{fig:bees} but can occur in multiple of the domains previously listed.
% Especially when each region can be arbitrarily complex, the software will enter a non-responsive state where no annotation can be performed.

\makeSampleSegFig
\makeBeesFig

% A potential workaround is to bootstrap machine learning models through transfer learning on a similar dataset and applying them on the current dataset.
% Manual supervision is then only required to verify the results are correct and make adjustments accordingly.
% While this approach is valid when existing datasets match the desired segmentation properties, it also means transfer learning is ineffective when training on novel data or image properties.
% Moreover, transfer learning is effective in assisting ground truth collection only when a sufficient data repository has already been gathered against which to validate network training \cite{opbroekTransferLearningImproves2015,weissSurveyTransferLearning2016}.

% Even after models are trained, it can be greatly beneficial to explore edge cases and pre/post-processing techniques while supervising the ground truth collection procedure.
%Toward this end, 
S3A circumvents these pitfalls by providing deep customization of the annotation workflow at both a granular and high level.
% To the best of our knowledge, it is the only application with this capability at both a granular and high level.
Arbitrary python functions can be seamlessly integrated into the processing workflow, and every algorithm developed within S3A can be accessed in a headless (i.e. automated or scripted) manner independent of the manual GUI.

In this paper, we present significant improvements to the fundamentals of S3A as previously introduced in \cite{jessurunComponentDetectionEvaluation2020}.
% Our revisions presented in this work are significant improvements to the previous publication's future work.
% In this paper, we present the core components of the S3A processing framework and labeling suite.
This includes live app-level property customization, real-time algorithm modification and feedback, region prediction assistance, constrained component table editing based on allowed data types, a multitude of data export formats, and a highly adaptable set of plugin interfaces for application-specific extensions to S3A. The front-end GUI  is shown in Figure~\ref{fig:appOverview}.
We also note the potential impact on the image processing community improving labeling speed, accuracy, and multiple insertion points for algorithm feedback.

\makeAppOverviewFig