


@MISC{SpaceNet2020-lb,
  title        = "{Multi-Temporal Urban Development Challenge}",
  author       = "{SpaceNet}",
  month        =  jun,
  year         =  2020,
  howpublished = "\url{https://spacenet.ai/sn7-challenge/}"
}


@article{ademImageProcessingBased2015,
  title = {Image Processing Based Quality Control of the Impermeable Seams in Multilayered Aseptic Packages},
  author = {Adem, Kemal and Orhan, Umut and Hekim, Mahmut},
  year = {2015},
  month = may,
  volume = {42},
  pages = {3785--3789},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2014.12.045},
  abstract = {Multilayered aseptic material which guarantees the extended shelf life of liquid foods is turned into an impermeable package by folding packaging machines. Some problems in the machines causes the packages to lose the impermeability property and therefore lead to deterioration of the liquid food. The leak test, which was performed to determine the problem, has been carried out by expert employees. The control process has steps of random selection of the liquid food packages, the opening of it properly, and observing the distribution of the injected ink in the seams by human eye. It is known that this control can cause serious material damage due to the human-based errors. With this study, it was aimed to perform the leak test of seams in multilayered aseptic packages by using a combination of the image processing and classification techniques. Through the data obtained from a real production environment, the experiments were performed and the results were evaluated. As a result, it can be said that the study has a distinctive feature as being the first in its field with its promising results.},
  file = {/home/ntjess/Zotero/storage/S4B347RU/Adem et al. - 2015 - Image processing based quality control of the impe.pdf;/home/ntjess/Zotero/storage/LE9JH7HZ/S0957417414008240.html},
  journal = {Expert Systems with Applications},
  keywords = {Canny edge detection,Image processing,Impermeable seams,Multilayered aseptic package,Pixel based image segmentation,Quality control},
  language = {en},
  number = {7}
}

@article{anagnostopoulosComputerVisionApproach2001,
  title = {A Computer Vision Approach for Textile Quality Control},
  author = {Anagnostopoulos, C. and Vergados, D. and Kayafas, E. and Loumos, V. and Stassinopoulos, G.},
  year = {2001},
  volume = {12},
  pages = {31--44},
  issn = {1099-1778},
  doi = {10.1002/vis.245},
  abstract = {Textile manufacturers have to monitor the quality of their products in order to maintain the high-quality standards established for the clothing industry. Thus, textile quality control is a key factor for the increase of competitiveness of their companies. Textile faults have traditionally been detected by human visual inspection. However, human inspection is time consuming and does not achieve a high level of accuracy. Therefore, industrial vision units are of strategic interest for the textile industry as they could form the basis of a system achieving a high degree of accuracy on textile inspection. This work describes the software core of a system designed for fabric inspection on the basis of simple image-processing operations as well as its efficiency on detection of usual textile defects. The prerequisites of the overall system are then discussed analytically, as well as the limitations and the restrictions imposed due to the nature of the problem. The software algorithm and the evaluation of the first results are also presented in details. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  file = {/home/ntjess/Zotero/storage/YIWDKWAL/Anagnostopoulos et al. - 2001 - A computer vision approach for textile quality con.pdf;/home/ntjess/Zotero/storage/4USC9MG7/vis.html},
  journal = {The Journal of Visualization and Computer Animation},
  keywords = {computer vision,fabric inspection system,image processing,quality control,textile,texture segmentation},
  language = {en},
  number = {1}
}

@article{anagnostopoulosHighPerformanceComputing2002,
  title = {High Performance Computing Algorithms for Textile Quality Control},
  author = {Anagnostopoulos, C and Anagnostopoulos, I and Vergados, D and Kouzas, G and Kayafas, E and Loumos, V and Stassinopoulos, G},
  year = {2002},
  month = sep,
  volume = {60},
  pages = {389--400},
  issn = {0378-4754},
  doi = {10.1016/S0378-4754(02)00031-9},
  abstract = {At the present time, industries like textile are in constant need of modernisation. Thus, their presence in the high technology area of high performance computing (HPC) based inspection is of strategic interest. Textile manufacturers have to monitor the quality of their products in order to maintain the high quality standards established for the textile industry. The scope of this paper is to present a HPC architecture which can be implemented at each step of the quality control process in fabrics. The prerequisites or the overall system are then discussed analytically, as well as the limitations and the restrictions imposed due to the nature of the problem. The software algorithm and the evaluation of the first results are also presented in detail.},
  file = {/home/ntjess/Zotero/storage/5RLZY3EV/Anagnostopoulos et al. - 2002 - High performance computing algorithms for textile .pdf;/home/ntjess/Zotero/storage/YWP5WRU5/S0378475402000319.html},
  journal = {Mathematics and Computers in Simulation},
  keywords = {Computer vision,High performance computing,Industry,Inspection system,Quality control,Textile},
  language = {en},
  number = {3},
  series = {Intelligent {{Forecasting}}, {{Fault Diagnosis}}, {{Scheduling}}, and {{Control}}}
}

@inproceedings{azhaganReviewAutomaticBill2019,
  title = {A Review on Automatic Bill of Material Generation and Visual Inspection on {{PCBs}}},
  booktitle = {{{ISTFA}} 2019: {{Proceedings}} of the 45th {{International Symposium}} for {{Testing}} and {{Failure Analysis}}},
  author = {Azhagan, Mukhil and Mehta, Dhwani and Lu, Hangwei and Agrawal, Sudarshan and Tehranipoor, Mark and Woodard, Damon L and Asadizanjani, Navid and Chawla, Praveen},
  year = {2019},
  pages = {256},
  publisher = {{ASM International}}
}

@misc{BestImageAnnotation,
title={The best image annotation platforms for computer vision (+ an honest review of each)},
url={https://hackernoon.com/the-best-image-annotation-platforms-for-computer-vision-an-honest-review-of-each-dac7f565fea}, journal={HACKERNOON}, author={Humans in the Loop}, year={2018}, month=Oct
}

@article{chengSurveyAnalysisAutomatic2018,
  title = {A Survey and Analysis on Automatic Image Annotation},
  author = {Cheng, Qimin and Zhang, Qian and Fu, Peng and Tu, Conghuan and Li, Sen},
  year = {2018},
  volume = {79},
  pages = {242--259},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2018.02.017},
  abstract = {In recent years, image annotation has attracted extensive attention due to the explosive growth of image data. With the capability of describing images at the semantic level, image annotation has many applications not only in image analysis and understanding but also in some relative disciplines, such as urban management and biomedical engineering. Because of the inherent weaknesses of manual image annotation, Automatic Image Annotation (AIA) has been raised since the late 1990s. In this paper, a deep review of state-of-the-art AIA methods is presented by synthesizing 138 literatures published during the past two decades. We classify AIA methods into five categories: 1) Generative model-based image annotation, 2) Nearest neighbor-based image annotation, 3) Discriminative model-based image annotation, and 4) Tag completion-based image annotation, 5) Deep Learning-based image annotation. Comparisons of the five types of AIA methods are made on the basis of the underlying idea, main contribution, model framework, computational complexity, computation time, and annotation accuracy. We also give an overview of five publicly available image datasets and four standard evaluation metrics commonly used as benchmarks for evaluating AIA methods. Then the performance of some typical or well-behaved models is assessed based on benchmark dataset and standard evaluation metrics. Finally, we share our viewpoints on the open issues and challenges in AIA as well as research trends in the future.},
  journal = {Pattern Recognition},
  keywords = {Automatic image annotation,Deep learning,Discriminative model,Generative model,Nearest-neighbor model,Tag-completion}
}

@incollection{dasiopoulouSurveySemanticImage2011,
  title = {A Survey of Semantic Image and Video Annotation Tools},
  booktitle = {Knowledge-{{Driven Multimedia Information Extraction}} and {{Ontology Evolution}}},
  author = {Dasiopoulou, Stamatia and Giannakidou, Eirini and Litos, Georgios and Malasioti, Polyxeni and Kompatsiaris, Yiannis},
  editor = {Paliouras, Georgios and Spyropoulos, Constantine D. and Tsatsaronis, George},
  year = {2011},
  volume = {6050},
  pages = {196--239},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-20795-2_8},
  file = {/home/ntjess/Zotero/storage/VZN22WD7/Dasiopoulou et al. - 2011 - A Survey of Semantic Image and Video Annotation To.pdf},
  isbn = {978-3-642-20794-5 978-3-642-20795-2}
}

@article{fergusonDetectionSegmentationManufacturing2018,
  title = {Detection and Segmentation of Manufacturing Defects with Convolutional Neural Networks and Transfer Learning},
  author = {Ferguson, Max K. and Ronay, Ak and Lee, Yung-Tsun Tina and Law, Kincho. H.},
  year = {2018},
  volume = {2},
  issn = {2572-3928},
  doi = {10.1520/SSMS20180033},
  abstract = {Quality control is a fundamental component of many manufacturing processes, especially those involving casting or welding. However, manual quality control procedures are often time-consuming and error-prone. In order to meet the growing demand for high-quality products, the use of intelligent visual inspection systems is becoming essential in production lines. Recently, Convolutional Neural Networks (CNNs) have shown outstanding performance in both image classification and localization tasks. In this article, a system is proposed for the identification of casting defects in X-ray images, based on the Mask Region-based CNN architecture. The proposed defect detection system simultaneously performs defect detection and segmentation on input images, making it suitable for a range of defect detection tasks. It is shown that training the network to simultaneously perform defect detection and defect instance segmentation, results in a higher defect detection accuracy than training on defect detection alone. Transfer learning is leveraged to reduce the training data demands and increase the prediction accuracy of the trained model. More specifically, the model is first trained with two large openly-available image datasets before finetuning on a relatively small metal casting X-ray dataset. The accuracy of the trained model exceeds state-of-the art performance on the GRIMA database of X-ray images (GDXray) Castings dataset and is fast enough to be used in a production setting. The system also performs well on the GDXray Welds dataset. A number of in-depth studies are conducted to explore how transfer learning, multi-task learning, and multi-class learning influence the performance of the trained system.},
  file = {/home/ntjess/Zotero/storage/P7PLGJQL/Ferguson et al. - 2018 - Detection and Segmentation of Manufacturing Defect.pdf},
  journal = {Smart and sustainable manufacturing systems},
  pmcid = {PMC6512995},
  pmid = {31093604}
}

@article{fujisawaSegmentationMethodsCharacter1992,
  title = {Segmentation Methods for Character Recognition: From Segmentation to Document Structure Analysis},
  shorttitle = {Segmentation Methods for Character Recognition},
  author = {Fujisawa, H. and Nakano, Y. and Kurino, K.},
  year = {1992},
  month = jul,
  volume = {80},
  pages = {1079--1092},
  issn = {1558-2256},
  doi = {10.1109/5.156471},
  abstract = {A pattern-oriented segmentation method for optical character recognition that leads to document structure analysis is presented. As a first example, segmentation of handwritten numerals that touch are treated. Connected pattern components are extracted, and spatial interrelations between components are measured and grouped into meaningful character patterns. Stroke shapes are analyzed and a method of finding the touching positions that separates about 95\% of connected numerals correctly is described. Ambiguities are handled by multiple hypotheses and verification by recognition. An extended form of pattern-oriented segmentation, tabular form recognition, is considered. Images of tabular forms are analyzed, and frames in the tabular structure are extracted. By identifying semantic relationships between label frames and data frames, information on the form can be properly recognized.{$<>$}},
  file = {/home/ntjess/Zotero/storage/PP9JUVZ7/156471.html},
  journal = {Proceedings of the IEEE},
  keywords = {character patterns,character recognition,Character recognition,connected pattern components,data frames,document image processing,document structure analysis,handwritten numerals,Image segmentation,label frames,multiple hypotheses,OCR,optical character recognition,Optical character recognition software,Paper technology,Pattern classification,Pattern recognition,Pixel,segmentation,semantic relationships,spatial interrelations,tabular form recognition,Text analysis,touching positions,Usability,Writing},
  number = {7}
}

@inproceedings{gatosSegmentationfreeRecognitionTechnique2004,
  title = {A Segmentation-Free Recognition Technique to Assist Old Greek Handwritten Manuscript {{OCR}}},
  booktitle = {Document {{Analysis Systems VI}}},
  author = {Gatos, Basilios and Ntzios, Kostas and Pratikakis, Ioannis and Petridis, Sergios and Konidaris, T. and Perantonis, Stavros J.},
  editor = {Marinai, Simone and Dengel, Andreas R.},
  year = {2004},
  pages = {63--74},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28640-0_7},
  abstract = {Recognition of old Greek manuscripts is essential for quick and efficient content exploitation of the valuable old Greek historical collections. In this paper, we focus on the problem of recognizing early Christian Greek manuscripts written in lower case letters. Based on the existence of hole regions in the majority of characters and character ligatures in these scripts, we propose a novel, segmentation-free, fast and efficient technique that assists the recognition procedure by tracing and recognizing the most frequently appearing characters or character ligatures. First, we detect hole regions that exist in the character body. Then, the protrusions in the outer contour outline of the connected components that contain the character hole regions are used for the classification of the area around holes to a specific character or a character ligature. The proposed method gives highly accurate results and offers great assistance to old Greek handwritten manuscript OCR.},
  file = {/home/ntjess/Zotero/storage/N9EHKIDH/Gatos et al. - 2004 - A Segmentation-Free Recognition Technique to Assis.pdf},
  isbn = {978-3-540-28640-0},
  keywords = {Feature Extraction Algorithm,Handwritten Character,Hole Region,Horizontal Mode,Lower Case Letter},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{hanburySurveyMethodsImage2008,
  title = {A Survey of Methods for Image Annotation},
  author = {Hanbury, Allan},
  year = {2008},
  month = oct,
  volume = {19},
  pages = {617--627},
  issn = {1045-926X},
  doi = {10.1016/j.jvlc.2008.01.002},
  abstract = {In order to evaluate automated image annotation and object recognition algorithms, ground truth in the form of a set of images correctly annotated with text describing each image is required. In this paper, three image annotation approaches are reviewed: free text annotation, keyword annotation and annotation based on ontologies. The practical aspects of image annotation are then considered. We discuss the creation of keyword vocabularies for use in automated image annotation evaluation. As direct manual annotation of images requires much time and effort, we also review various methods to make the creation of ground truth more efficient. An overview of annotated image datasets for computer vision research is provided.},
  file = {/home/ntjess/Zotero/storage/TVEBKM38/Hanbury - 2008 - A survey of methods for image annotation.pdf;/home/ntjess/Zotero/storage/W962SH96/S1045926X08000037.html},
  journal = {Journal of Visual Languages \& Computing},
  keywords = {Algorithm evaluation,Computer vision,Image annotation,Object recognition,Ontology},
  language = {en},
  number = {5}
}

@inproceedings{hollinkEvaluatingApplicationSemantic2005,
  title = {Evaluating the Application of Semantic Inferencing Rules to Image Annotation},
  booktitle = {Proceedings of the 3rd International Conference on {{Knowledge}} Capture},
  author = {Hollink, L. and Little, S. and Hunter, J.},
  year = {2005},
  month = oct,
  pages = {91--98},
  publisher = {{Association for Computing Machinery}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1088622.1088639},
  abstract = {Semantic annotation of digital objects within large multimedia collections is a difficult and challenging task. We describe a method for semi-automatic annotation of images and apply it to and evaluate it on images of pancreatic cells. By comparing the performance of this approach in the pancreatic cell domain with previous results in the fuel cell domain, we aim to determine characteristics of a domain which indicate that the method will or will not work in that domain. We conclude by describing the types of images and domains in which we can expect satisfactory results with this approach.},
  file = {/home/ntjess/Zotero/storage/5TXDHWDL/Hollink et al. - 2005 - Evaluating the application of semantic inferencing.pdf},
  isbn = {978-1-59593-163-4},
  keywords = {evaluation,image annotation,inferencing rules},
  series = {K-{{CAP}} '05}
}

@misc{iakovidisRatsnakeVersatileImage2014,
  title = {Ratsnake: A Versatile Image Annotation Tool with Application to Computer-Aided Diagnosis},
  shorttitle = {Ratsnake},
  author = {Iakovidis, D. K. and Goudas, T. and Smailis, C. and Maglogiannis, I.},
  year = {2014},
  doi = {10.1155/2014/286856},
  abstract = {Image segmentation and annotation are key components of image-based medical computer-aided diagnosis (CAD) systems. In this paper we present Ratsnake, a publicly available generic image annotation tool providing annotation efficiency, semantic awareness, versatility, and extensibility, features that can be exploited to transform it into an effective CAD system. In order to demonstrate this unique capability, we present its novel application for the evaluation and quantification of salient objects and structures of interest in kidney biopsy images. Accurate annotation identifying and quantifying such structures in microscopy images can provide an estimation of pathogenesis in obstructive nephropathy, which is a rather common disease with severe implication in children and infants. However a tool for detecting and quantifying the disease is not yet available. A machine learning-based approach, which utilizes prior domain knowledge and textural image features, is considered for the generation of an image force field customizing the presented tool for automatic evaluation of kidney biopsy images. The experimental evaluation of the proposed application of Ratsnake demonstrates its efficiency and effectiveness and promises its wide applicability across a variety of medical imaging domains.},
  file = {/home/ntjess/Zotero/storage/K7YN2494/Iakovidis et al. - 2014 - Ratsnake A Versatile Image Annotation Tool with A.pdf;/home/ntjess/Zotero/storage/VBXZ2DGE/286856.html},
  journal = {The Scientific World Journal},
  language = {en},
  type = {Research {{Article}}}
}

@article{jainTextSegmentationUsing1992,
  title = {Text Segmentation Using Gabor Filters for Automatic Document Processing},
  author = {Jain, Anil K. and Bhattacharjee, Sushil},
  year = {1992},
  month = jun,
  volume = {5},
  pages = {169--184},
  issn = {1432-1769},
  doi = {10.1007/BF02626996},
  abstract = {There is a considerable interest in designing automatic systems that will scan a given paper document and store it on electronic media for easier storage, manipulation, and access. Most documents contain graphics and images in addition to text. Thus, the document image has to be segmented to identify the text regions, so that OCR techniques may be applied only to those regions. In this paper, we present a simple method for document image segmentation in which text regions in a given document image are automatically identified. The proposed segmentation method for document images is based on a multichannel filtering approach to texture segmentation. The text in the document is considered as a textured region. Nontext contents in the document, such as blank spaces, graphics, and pictures, are considered as regions with different textures. Thus, the problem of segmenting document images into text and nontext regions can be posed as a texture segmentation problem. Two-dimensional Gabor filters are used to extract texture features for each of these regions. These filters have been extensively used earlier for a variety of texture segmentation tasks. Here we apply the same filters to the document image segmentation problem. Our segmentation method does not assume any a priori knowledge about the content or font styles of the document, and is shown to work even for skewed images and handwritten text. Results of the proposed segmentation method are presented for several test images which demonstrate the robustness of this technique.},
  file = {/home/ntjess/Zotero/storage/GYK6D547/Jain and Bhattacharjee - 1992 - Text segmentation using gabor filters for automati.pdf},
  journal = {Machine Vision and Applications},
  language = {en},
  number = {3}
}

@inproceedings{kesimanNewSchemeText2016,
  title = {A New Scheme for Text Line and Character Segmentation from Gray Scale Images of Palm Leaf Manuscript},
  booktitle = {2016 15th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}} ({{ICFHR}})},
  author = {Kesiman, Made Windu Antara and Burie, Jean-Christophe and Ogier, Jean-Marc},
  year = {2016},
  month = oct,
  pages = {325--330},
  issn = {2167-6445},
  doi = {10.1109/ICFHR.2016.0068},
  abstract = {Most of text line and character segmentation methods for handwritten document image basically still depend on the binary image of the document. Unfortunately, for palm leaf manuscript images, the binarization process is a real challenge. We proposed a new binarization free scheme for text line and character segmentation for palm leaf manuscript images. Our scheme consists of 4 sub-tasks: brushing character area of gray level images with minimum filtering, average block projection profile of gray level images, selection of candidate area for segmentation path, and construction of nonlinear segmentation path. For evaluation, we compared our method with the shredding method which is applied in three different schemes of experiment. The experimental results showed that the proposed method performed optimal on the palm leaf manuscript images which contain discolored parts, with low intensity variations or poor contrast, random noises, and fading.},
  file = {/home/ntjess/Zotero/storage/AW5763US/Kesiman et al. - 2016 - A New Scheme for Text Line and Character Segmentat.pdf;/home/ntjess/Zotero/storage/7NWAEYC5/7814084.html},
  keywords = {average block projection profile,Balinese script,binarization free scheme,Brushes,brushing character area,character segmentation,Compounds,discolored parts,document image processing,Filtering,gray scale images,grey systems,handwritten character recognition,handwritten document image,image,image filtering,image segmentation,Image segmentation,Indexing,minimum image filtering,nonlinear segmentation path construction,optical character recognition,Optical character recognition software,palm leaf manuscript,palm leaf manuscript images,segmentation path selection,shredding method,text line segmentation,Text recognition}
}

@inproceedings{liaoSemanticAnnotationModel2011,
  title = {Semantic Annotation Model Definition for Systems Interoperability},
  booktitle = {On the {{Move}} to {{Meaningful Internet Systems}}: {{OTM}} 2011 {{Workshops}}},
  author = {Liao, Yongxin and Lezoche, Mario and Panetto, Herv{\'e} and Boudjlida, Nacer},
  editor = {Meersman, Robert and Dillon, Tharam and Herrero, Pilar},
  year = {2011},
  pages = {61--70},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {Semantic annotation is one of the useful solutions to enrich target's (systems, models, meta-models, etc.) information. There are some papers which use semantic enrichment for different purposes (integration, composition, sharing and reuse, etc.) in several domains, but none of them provides a complete process of how to use semantic annotations. This paper identifies three main components of semantic annotation, proposes for it a formal definition and presents a survey of current semantic annotation methods. At the end, we present a simple case study to explain how our semantic annotation proposition can be applied.The survey presented in this paper will be the basis of our future research on models, semantics and architecture for enterprises systems interoperability during the product lifecycle.},
  isbn = {978-3-642-25126-9}
}

@inproceedings{papageorgiouGeneralFrameworkObject1998,
  title = {A General Framework for Object Detection},
  booktitle = {Sixth {{International Conference}} on {{Computer Vision}} ({{IEEE Cat}}. {{No}}.{{98CH36271}})},
  author = {Papageorgiou, C.P. and Oren, M. and Poggio, T.},
  year = {1998},
  pages = {555--562},
  publisher = {{Narosa Publishing House}},
  address = {{Bombay, India}},
  doi = {10.1109/ICCV.1998.710772},
  isbn = {978-81-7319-221-0}
}

@article{rajchlDeepCutObjectSegmentation2017,
  title = {{{DeepCut}}: Object Segmentation from Bounding Box Annotations Using Convolutional Neural Networks},
  shorttitle = {{{DeepCut}}},
  author = {Rajchl, Martin and Lee, Matthew C. H. and Oktay, Ozan and Kamnitsas, Konstantinos and {Passerat-Palmbach}, Jonathan and Bai, Wenjia and Damodaram, Mellisa and Rutherford, Mary A. and Hajnal, Joseph V. and Kainz, Bernhard and Rueckert, Daniel},
  year = {2017},
  month = feb,
  volume = {36},
  pages = {674--683},
  issn = {1558-254X},
  doi = {10.1109/TMI.2016.2621185},
  abstract = {In this paper, we propose DeepCut, a method to obtain pixelwise object segmentations given an image dataset labelled weak annotations, in our case bounding boxes. It extends the approach of the well-known GrabCut[1] method to include machine learning by training a neural network classifier from bounding box annotations. We formulate the problem as an energy minimisation problem over a densely-connected conditional random field and iteratively update the training targets to obtain pixelwise object segmentations. Additionally, we propose variants of the DeepCut method and compare those to a na\"ive approach to CNN training under weak supervision. We test its applicability to solve brain and lung segmentation problems on a challenging fetal magnetic resonance dataset and obtain encouraging results in terms of accuracy.},
  file = {/home/ntjess/Zotero/storage/SHCADKGJ/Rajchl et al. - 2017 - DeepCut Object Segmentation From Bounding Box Ann.pdf;/home/ntjess/Zotero/storage/RRTHC755/7739993.html},
  journal = {IEEE Transactions on Medical Imaging},
  keywords = {Algorithms,Biological neural networks,biomedical MRI,Bounding box,bounding box annotations,brain,Brain,brain segmentation,Computational modeling,convolutional neural networks,DeepCut,densely connected conditional random field,fetal magnetic resonance dataset,GrabCut,Humans,Image Enhancement,Image Interpretation; Computer-Assisted,image segmentation,Image segmentation,Imaging,learning (artificial intelligence),lung,lung segmentation,machine learning,Machine Learning,Magnetic Resonance Imaging,medical image processing,Monte Carlo Method,neural nets,Neural Networks (Computer),Object segmentation,Optimization,pixelwise object segmentation,Training,weak annotations},
  number = {2}
}

@inproceedings{seifertSemanticAnnotationMedical2010,
  author = {Sascha Seifert and Michael Kelm and Manuel Moeller and Saikat Mukherjee and Alexander Cavallaro and Martin Huber and Dorin Comaniciu},
  title = {{Semantic annotation of medical images}},
  volume = {7628},
  booktitle = {Medical Imaging 2010: Advanced PACS-based Imaging Informatics and Therapeutic Applications},
  editor = {Brent J. Liu and William W. Boonn},
  organization = {International Society for Optics and Photonics},
  publisher = {SPIE},
  pages = {43 -- 50},
  keywords = {image parsing, ontological modeling, semantic image annotation},
  year = {2010},
  doi = {10.1117/12.844207},
  URL = {https://doi.org/10.1117/12.844207}
}

@article{taxtSegmentationDocumentImages1989,
  title = {Segmentation of Document Images},
  author = {Taxt, T. and Flynn, P.J. and Jain, A.K.},
  year = {1989},
  month = dec,
  volume = {11},
  pages = {1322--1329},
  issn = {1939-3539},
  doi = {10.1109/34.41371},
  abstract = {Several methods for segmentation of document images (maps, drawings, etc.) are explored. The segmentation operation is posed as a statistical classification task with two pattern classes: print and background. A number of classification strategies are available. All require some prior information about the distribution of gray levels for the two classes. Training (either supervised or unsupervised) is employed to form these initial density estimates. Automatic updating of the class-conditional densities is performed within subregions in the image to adapt these global density estimates to the local image area. After local class-conditional densities have been obtained, each pixel is classified within the window using several techniques: a noncontextual Bayes classifier, Besag's classifier, relaxation, Owen and Switzer's classifier, and Haslett's classifier. Four test images were processed. In two of these, the relaxation method performed best, and in the other two, the noncontextual method performed best. Automatic updating improved the results for both classifiers.{$<>$}},
  file = {/home/ntjess/Zotero/storage/78IPAHP2/Taxt et al. - 1989 - Segmentation of document images.pdf;/home/ntjess/Zotero/storage/9JTZ8NWJ/41371.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {background,Besag's classifier,class-conditional densities,Councils,Data mining,Degradation,Digital images,document image segmentation,drawings,Fading,gray level distribution,Haslett's classifier,Image databases,Image segmentation,maps,Markov random fields,noncontextual Bayes classifier,Owen and Switzer's classifier,pattern recognition,picture processing,print,relaxation,statistical analysis,statistical classification task,Storage automation,Testing},
  number = {12}
}

@article{yushkevichUserguided3DActive2006,
  title = {User-Guided {{3D}} Active Contour Segmentation of Anatomical Structures: Significantly Improved Efficiency and Reliability},
  shorttitle = {User-Guided {{3D}} Active Contour Segmentation of Anatomical Structures},
  author = {Yushkevich, Paul A. and Piven, Joseph and Hazlett, Heather Cody and Smith, Rachel Gimpel and Ho, Sean and Gee, James C. and Gerig, Guido},
  year = {2006},
  month = jul,
  volume = {31},
  pages = {1116--1128},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2006.01.015},
  abstract = {Active contour segmentation and its robust implementation using level set methods are well-established theoretical approaches that have been studied thoroughly in the image analysis literature. Despite the existence of these powerful segmentation methods, the needs of clinical research continue to be fulfilled, to a large extent, using slice-by-slice manual tracing. To bridge the gap between methodological advances and clinical routine, we developed an open source application called ITK-SNAP, which is intended to make level set segmentation easily accessible to a wide range of users, including those with little or no mathematical expertise. This paper describes the methods and software engineering philosophy behind this new tool and provides the results of validation experiments performed in the context of an ongoing child autism neuroimaging study. The validation establishes SNAP intrarater and interrater reliability and overlap error statistics for the caudate nucleus and finds that SNAP is a highly reliable and efficient alternative to manual tracing. Analogous results for lateral ventricle segmentation are provided.},
  file = {/home/ntjess/Zotero/storage/SJZHNDN5/Yushkevich et al. - 2006 - User-guided 3D active contour segmentation of anat.pdf;/home/ntjess/Zotero/storage/I7P5PXQR/S1053811906000632.html},
  journal = {NeuroImage},
  keywords = {3D active contour models,Anatomical objects,Caudate nucleus,Computational anatomy,Image segmentation,Open source software,Validation},
  language = {en},
  number = {3}
}


@article{jessurunComponentDetectionEvaluation2020,
  title = {Component {{Detection}} and {{Evaluation Framework}} ({{CDEF}}): {{A Semantic Annotation Tool}}},
  shorttitle = {Component {{Detection}} and {{Evaluation Framework}} ({{CDEF}})},
  author = {Jessurun, Nathan and Paradis, Olivia and Roberts, Alexandra and Asadizanjani, Navid},
  year = {2020},
  month = aug,
  volume = {26},
  pages = {1470--1474},
  issn = {1431-9276, 1435-8115},
  doi = {10.1017/S1431927620018243},
  abstract = {Abstract             Image labeling is the process of manually assigning a class to subregions within an image for machine learning applications. When these subregions are complex shapes, this process is known as semantic segmentation. We propose a new software application, the Component Detection and Evaluation Framework (CDEF), for creating such semantic labels. The benefits of CDEF over existing tools are highlighted, and further improvements are proposed.},
  file = {/home/ntjess/Zotero/storage/Y7KHQFXJ/Jessurun et al. - 2020 - Component Detection and Evaluation Framework (CDEF.pdf},
  journal = {Microscopy and Microanalysis},
  language = {en},
  number = {S2}
}



@article{opbroekTransferLearningImproves2015,
  title = {Transfer {{Learning Improves Supervised Image Segmentation Across Imaging Protocols}}},
  author = {van Opbroek, A. and Ikram, M. A. and Vernooij, M. W. and de Bruijne, M.},
  year = {2015},
  month = may,
  volume = {34},
  pages = {1018--1030},
  issn = {1558-254X},
  doi = {10.1109/TMI.2014.2366792},
  abstract = {The variation between images obtained with different scanners or different imaging protocols presents a major challenge in automatic segmentation of biomedical images. This variation especially hampers the application of otherwise successful supervised-learning techniques which, in order to perform well, often require a large amount of labeled training data that is exactly representative of the target data. We therefore propose to use transfer learning for image segmentation. Transfer-learning techniques can cope with differences in distributions between training and target data, and therefore may improve performance over supervised learning for segmentation across scanners and scan protocols. We present four transfer classifiers that can train a classification scheme with only a small amount of representative training data, in addition to a larger amount of other training data with slightly different characteristics. The performance of the four transfer classifiers was compared to that of standard supervised classification on two magnetic resonance imaging brain-segmentation tasks with multi-site data: white matter, gray matter, and cerebrospinal fluid segmentation; and white-matter-/MS-lesion segmentation. The experiments showed that when there is only a small amount of representative training data available, transfer learning can greatly outperform common supervised-learning approaches, minimizing classification errors by up to 60\%.},
  file = {/home/ntjess/Zotero/storage/J4SMTUGW/Opbroek et al. - 2015 - Transfer Learning Improves Supervised Image Segmen.pdf;/home/ntjess/Zotero/storage/HFQ8UJH6/6945865.html},
  journal = {IEEE Transactions on Medical Imaging},
  keywords = {automatic segmentation,biomedical images,Biomedical imaging,biomedical MRI,brain,Brain,cerebrospinal fluid segmentation,classification scheme,gray matter,Humans,image classification,Image Processing; Computer-Assisted,image segmentation,Image segmentation,Image Segmentation,image variation,imaging protocols,Kernel,labeled training data,learning (artificial intelligence),machine learning,Machine Learning,magnetic resonance imaging,Magnetic Resonance Imaging,magnetic resonance imaging brain-segmentation tasks,medical image processing,minimizing classification errors,Multiple Sclerosis,multisite data,neurophysiology,pattern recognition,Pattern Recognition; Automated,Protocols,representative training data,standard supervised classification,supervised image segmentation,supervised-learning techniques,Support Vector Machine,Support vector machines,target data distributions,Training,Training data,transfer classifiers,transfer learning,white-matter-MS-lesion segmentation},
  number = {5}
}

@article{weissSurveyTransferLearning2016,
  title = {A Survey of Transfer Learning},
  author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
  year = {2016},
  month = may,
  volume = {3},
  pages = {9},
  issn = {2196-1115},
  doi = {10.1186/s40537-016-0043-6},
  abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
  file = {/home/ntjess/Zotero/storage/HZDJ6F3C/Weiss et al. - 2016 - A survey of transfer learning.pdf;/home/ntjess/Zotero/storage/W2RI5L6H/s40537-016-0043-6.html},
  journal = {Journal of Big Data},
  keywords = {Data mining,Domain adaptation,Machine learning,Survey,Transfer learning},
  number = {1}
}

@InProceedings{Ladicky_whatWhereCombiningCRFs,
author="Ladick{\'y}, {\v{L}}ubor
and Sturgess, Paul
and Alahari, Karteek
and Russell, Chris
and Torr, Philip H. S.",
editor="Daniilidis, Kostas
and Maragos, Petros
and Paragios, Nikos",
title="What, Where and How Many? Combining Object Detectors and CRFs",
booktitle="Computer Vision -- ECCV 2010",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="424--437",
abstract="Computer vision algorithms for individual tasks such as object recognition, detection and segmentation have shown impressive results in the recent past. The next challenge is to integrate all these algorithms and address the problem of scene understanding. This paper is a step towards this goal. We present a probabilistic framework for reasoning about regions, objects, and their attributes such as object class, location, and spatial extent. Our model is a Conditional Random Field defined on pixels, segments and objects. We define a global energy function for the model, which combines results from sliding window detectors, and low-level pixel-based unary and pairwise relations. One of our primary contributions is to show that this energy function can be solved efficiently. Experimental results show that our model achieves significant improvement over the baseline methods on CamVid and pascal voc datasets.",
isbn="978-3-642-15561-1"
}


@InProceedings{Branson_humansInLoop,
author="Branson, Steve
and Wah, Catherine
and Schroff, Florian
and Babenko, Boris
and Welinder, Peter
and Perona, Pietro
and Belongie, Serge",
editor="Daniilidis, Kostas
and Maragos, Petros
and Paragios, Nikos",
title="Visual Recognition with Humans in the Loop",
booktitle="Computer Vision -- ECCV 2010",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="438--451",
abstract="We present an interactive, hybrid human-computer method for object classification. The method applies to classes of objects that are recognizable by people with appropriate expertise (e.g., animal species or airplane model), but not (in general) by people without such expertise. It can be seen as a visual version of the 20 questions game, where questions based on simple visual attributes are posed interactively. The goal is to identify the true class while minimizing the number of questions asked, using the visual content of the image. We introduce a general framework for incorporating almost any off-the-shelf multi-class object recognition algorithm into the visual 20 questions game, and provide methodologies to account for imperfect user responses and unreliable computer vision algorithms. We evaluate our methods on Birds-200, a difficult dataset of 200 tightly-related bird species, and on the Animals With Attributes dataset. Our results demonstrate that incorporating user input drives up recognition accuracy to levels that are good enough for practical applications, while at the same time, computer vision reduces the amount of human interaction required.",
isbn="978-3-642-15561-1"
}

@InProceedings{Dai_crowdSourceWorkflows,
	author = {Dia, Peng and Mausam and Weld, Daniel},
	title = {Decision-Theoretic Control of Crowd-Sourced Workflows},
	booktitle = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2010},
	keywords = {},
	abstract = {Crowd-sourcing is a recent framework in which human intelligence tasks are outsourced to a crowd of unknown people ("workers") as an open call (e.g., on Amazon's Mechanical Turk). Crowd-sourcing has become immensely popular with hoards of employers ("requesters"), who use it to solve a wide variety of jobs, such as dictation transcription, content screening, etc. In order to achieve quality results, requesters often subdivide a large task into a chain of bite-sized subtasks that are combined into a complex, iterative workflow in which workers check and improve each other's results. This paper raises an exciting question for AI — could an autonomous agent control these workflows without human intervention, yielding better results than today's state of the art, a fixed control program?  We describe a planner, TurKontrol, that formulates workflow control as a decision-theoretic optimization problem, trading off the implicit quality of a solution artifact against the cost for workers to achieve it. We lay the mathematical framework to govern the various decisions at each point in a popular class of workflows. Based on our analysis we implement the workflow control algorithm and present experiments demonstrating that TurKontrol obtains much higher utilities than popular fixed policies.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1873}
}

@inproceedings{Wang_multiLabelImageAnnotation,
 title={Multi-label sparse coding for automatic image annotation},
 ISSN={1063-6919},
 DOI={10.1109/CVPR.2009.5206866},
 booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
 author={Wang, C. and Shuicheng Yan and Lei Zhang and Zhang, H.},
 year={2009},
 month=Jun,
 pages={1643–1650} }
 
 @inproceedings{Russakovsky_humanCollabAnnotation2015,
 place={Boston,
 MA,
 USA},
 title={Best of both worlds: Human-machine collaboration for object annotation},
 ISBN={9781467369640},
 url={http://ieeexplore.ieee.org/document/7298824/},
 DOI={10.1109/CVPR.2015.7298824},
 booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 publisher={IEEE},
 author={Russakovsky,  Olga and Li,  Li-Jia and Fei-Fei,  Li},
 year={2015},
 month=Jun,
 pages={2121–2131}}

@article{Radenovic_CNNNoHuman,
 title={Fine-Tuning CNN Image Retrieval with No Human Annotation},
 volume={41},
 ISSN={1939-3539},
 DOI={10.1109/TPAMI.2018.2846566},
 number={7},
 journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
 author={Radenović,
 F. and Tolias,
 G. and Chum,
 O.},
 year={2019},
 month={Jul},
 pages={1655–1668} }

@article{Vigueras_fullCnnCornealSegmentation,
 title={Fully convolutional architecture vs sliding-window CNN for corneal endothelium cell segmentation},
 volume={1},
 ISSN={2524-4426},
 DOI={10.1186/s42490-019-0003-2},
 number={1},
 journal={BMC Biomedical Engineering},
 author={Vigueras-Guillén,
 Juan P. and Sari,
 Busra and Goes,
 Stanley F. and Lemij,
 Hans G. and van Rooij,
 Jeroen and Vermeer,
 Koenraad A. and van Vliet,
 Lucas J.},
 year={2019},
 month={Jan},
 pages={4} }

@inproceedings{Mohajerani_cloudRemoteSensing,
 title={A Cloud Detection Algorithm for Remote Sensing Images Using Fully Convolutional Neural Networks},
 ISSN={2473-3628},
 DOI={10.1109/MMSP.2018.8547095},
 booktitle={2018 IEEE 20th International Workshop on Multimedia Signal Processing (MMSP)},
 author={Mohajerani,
 S. and Krammer,
 T. A. and Saeedi,
 P.},
 year={2018},
 month={Aug},
 pages={1–5} }

@inproceedings{Demochkina_improvingOneShotXray,
 place={Sochi,
 Russia},
 title={Improving the Accuracy of One-Shot Detectors for Small Objects in X-ray Images},
 ISBN={9781728161303},
 url={https://ieeexplore.ieee.org/document/9208097/},
 DOI={10.1109/RusAutoCon49822.2020.9208097},
 booktitle={2020 International Russian Automation Conference (RusAutoCon)},
 publisher={IEEE},
 author={Demochkina,
 Polina and Savchenko,
 Andrey V.},
 year={2020},
 month={Sep},
 pages={610–614} }


@inproceedings{paradis2020color,
title = {Color Normalization for Robust Automatic Bill of Materials Generation and Visual Inspection of PCBs},
author = {Olivia P Paradis and Nathan T Jessurun and Mark Tehranipoor and Navid Asadizanjani},
url = {https://doi.org/10.31399/asm.cp.istfa2020p0172
https://dl.asminternational.org/istfa/proceedings-pdf/ISTFA2020/83348/172/425605/istfa2020p0172.pdf},
doi = {10.31399/asm.cp.istfa2020p0172},
year = {2020},
date = {2020-01-01},
booktitle = {ISTFA 2020: Papers Accepted for the Planned 46th International Symposium for Testing and Failure Analysis},
pages = {172-179},
series = {International Symposium for Testing and Failure Analysis},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings}
}